# llama cpp

## 기본 설명

1. llama.cpp는 Meta의 LLaMA, 그리고 Llama 2와 Llama 3 같은 대형 언어 모델을 일반 컴퓨터에서도 효율적으로 실행할 수 있게 해주는 C/C++ 포트입니다. Georgi Gerganov에 의해 개발된 이 프로젝트는 고성능 GPU나 클라우드 서비스 없이도 소비자급 하드웨어에서 강력한 AI 모델을 구동할 수 있도록 설계되었습니다.

- llama.cpp는 낮은 사양의 하드웨어에서도 실행 가능하게 함으로써, AI 연구와 응용을 더 많은 개발자와 연구자에게 접근 가능하게 만듭니다.

## 주요 특징

llama.cpp는 다음과 같은 주요 특징을 제공합니다:
1. 효율적인 메모리 사용

- 양자화: 모델 가중치를 더 작은 비트 폭으로 압축해 메모리 요구사항을 크게 감소시킵니다
- 메모리 매핑: 디스크에서 직접 모델을 로드하여 RAM 사용량을 최소화합니다

2. 다양한 하드웨어 지원

- CPU 최적화: AVX, AVX2, AVX-512 같은 고급 벡터 명령어 지원
- GPU 가속: CUDA(NVIDIA), ROCm(AMD), Metal(Apple) 등 다양한 GPU 백엔드 지원
- 멀티 디바이스: CPU와 GPU 간의 작업 분배 가능

3. 유연한 인터페이스

- 명령줄 인터페이스: 간단한 텍스트 생성부터 대화형 채팅까지
- HTTP 서버: 웹 인터페이스와 REST API 제공
- C/C++ API: 다른 애플리케이션에 통합 가능
- Python 바인딩: Python 프로젝트에서 쉽게 사용 가능

4. 다양한 모델 지원

- GGUF 포맷: 효율적인 모델 저장 및 로딩을 위한 포맷
- 다양한 모델 호환성: Llama, Llama 2, Llama 3, Mistral, Vicuna 등 다양한 모델 지원
- LoRA 어댑터: 파인튜닝된 모델 지원

## 작동 원리
llama.cpp는 몇 가지 핵심 기술을 활용하여 고성능을 달성합니다:

1. 양자화(Quantization)
양자화는 모델 가중치의 정밀도를 낮추어 메모리 사용량을 줄이는 기술입니다. llama.cpp는 다양한 양자화 방식을 제공합니다:

- q4_0: 4비트 정수 양자화, 가장 메모리 효율적이나 품질은 다소 저하
- q4_1: 4비트 정수 + 작은 오프셋, q4_0보다 향상된 품질
- q5_0, q5_1: 5비트 양자화, 더 나은 품질과 합리적인 메모리 사용
- q8_0: 8비트 양자화, 원본에 근접한 품질이나 더 많은 메모리 필요
- q2_K, q3_K, q4_K, q5_K, q6_K, q8_K: K-퀀타이저를 사용한 양자화, 작은 그룹에 개별 스케일 적용
- q4_K_M, q5_K_M, q6_K_M, q8_K_M: K-퀀타이저 + 혼합 품질, 향상된 품질/크기 균형

2. 대략적인 메모리 요구사항:

- 7B 모델: ~원본 28GB → 양자화 후 4-7GB
- 13B 모델: ~원본 52GB → 양자화 후 8-13GB
- 70B 모델: ~원본 280GB → 양자화 후 40-70GB

3. 컴퓨팅 최적화
llama.cpp는 여러 가지 컴퓨팅 최적화 기술을 사용합니다:

- SIMD 명령어 활용: AVX, AVX2, AVX-512와 같은 벡터 명령어로 병렬 계산 가속화
- 커널 최적화: 행렬 곱셈과 같은 핵심 연산을 하드웨어에 최적화
- 메모리 레이아웃 최적화: 캐시 효율적인 데이터 구조 사용
- 멀티스레딩: 여러 CPU 코어를 활용한 병렬 처리
- GPU 오프로딩: 계산 집약적인 작업을 GPU로 전달

4. 추론 흐름
llama.cpp에서 텍스트 생성 과정:

- 토큰화: 입력 텍스트를 토큰 ID로 변환
- KV 캐시 초기화: 키-값 캐시 준비
- 컨텍스트 처리: 모든 입력 토큰을 한 번에 처리하여 KV 캐시 구축
- 자동 회귀 생성: 이전 토큰을 기반으로 다음 토큰 생성
- 샘플링: 온도, 빔 검색, 상위-p, 상위-k 등의 전략으로 다음 토큰 선택
- 토큰 디코딩: 생성된 토큰 ID를 텍스트로 변환
- 반복: 원하는 길이의 텍스트가 생성될 때까지 4-6단계 반복